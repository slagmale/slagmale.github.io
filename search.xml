<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis5大常用数据类型]]></title>
    <url>%2F2018%2F04%2F30%2FRedis5%E5%A4%A7%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[存储格式 基本用法通过Jedis(封装了redis的Java客户端)对redis进行操作。 Jedis工具类1234567891011121314151617181920212223242526272829303132public class JedisPoolUtil &#123; private static JedisPool pool = null; static &#123; //加载配置文件 InputStream in = JedisPoolUtil.class.getClassLoader().getResourceAsStream("redis.properties"); Properties pro = new Properties(); try &#123; pro.load(in); &#125; catch (IOException e) &#123; e.printStackTrace(); System.out.println("加载文件失败"); &#125; JedisPoolConfig poolConfig = new JedisPoolConfig(); //最大连接数 poolConfig.setMaxTotal(Integer.parseInt( pro.get("redis.maxTotal").toString())); //最大空闲连接数 poolConfig.setMaxIdle(Integer.parseInt( pro.get("redis.maxIdle").toString())); //最小空闲连接数 poolConfig.setMinIdle(Integer.parseInt( pro.get("redis.minIdle").toString())); pool = new JedisPool(poolConfig, pro.get("redis.url").toString(),Integer.parseInt( pro.get("redis.port") .toString())); &#125; public static Jedis getJedis()&#123; return pool.getResource(); &#125; public static void release(Jedis jedis)&#123; if(null != jedis)&#123; jedis.close(); &#125; &#125;&#125; redis配置文件12345redis.maxTotal=100redis.maxIdle=30redis.minIdle=10redis.url=192.168.202.200 redis.port=6379 String123456789101112131415161718192021222324252627282930313233343536373839404142434445public class StringTest &#123; public Jedis jedis = JedisPoolUtil.getJedis(); @Test //添加和获取 public void fun()&#123; jedis.set("num","1"); System.out.println(jedis.get("num")); &#125; @Test //删除值 public void fun1()&#123; jedis.del("num"); System.out.println(jedis.get("num")); &#125; @Test //自减和自减 public void fun2()&#123; jedis.set("num","1"); System.out.println(jedis.get("num")); jedis.decr("num"); System.out.println(jedis.get("num")); jedis.incr("num"); jedis.incr("num"); System.out.println(jedis.get("num")); &#125; @Test //加上/减去 一个数 //incrBy 返回的是修改之后的值如果原值是字符串不是数字，则会抛出异常 public void fun3()&#123; Long num = jedis.incrBy("num", 3); System.out.println(num); jedis.decrBy("num",10); System.out.println(jedis.get("num")); jedis.set("name","caopengfei"); //jedis.decrBy("name",1); &#125; @Test //字符串拼接 public void fun4()&#123; Long len = jedis.append("name", "123"); System.out.println(len); System.out.println(jedis.get("name")); &#125;&#125; Hash1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class HashTest &#123; public Jedis jedis = JedisPoolUtil.getJedis(); // hash 操作的是map对象// 适合存储键值对象的信息 @Test //存值 参数第一个变量的名称， map键名(key)， map键值(value)// 调用hset public void fun() &#123; Long num = jedis.hset("hash1", "username", "caopengfei"); System.out.println(num); String hget = jedis.hget("hash1", "username"); System.out.println(hget); &#125; @Test //也可以存多个key// 调用hmset public void fun1() &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("username", "caopengfei"); map.put("age", "25"); map.put("sex", "男"); String res = jedis.hmset("hash2", map); System.out.println(res);//ok &#125; @Test //获取hash中所有的值 public void fun2() &#123; Map&lt;String, String&gt; map2 = new HashMap&lt;String, String&gt;(); map2 = jedis.hgetAll("hash2"); System.out.println(map2); &#125; @Test// 删除hash中的键 可以删除一个也可以删除多个，返回的是删除的个数 public void fun3() &#123; Long num = jedis.hdel("hash2", "username", "age"); System.out.println(num); Map&lt;String, String&gt; map2 = new HashMap&lt;String, String&gt;(); map2 = jedis.hgetAll("hash2"); System.out.println(map2); &#125; @Test //增加hash中的键值对 public void fun4() &#123; Map&lt;String, String&gt; map2 = new HashMap&lt;String, String&gt;(); map2 = jedis.hgetAll("hash2"); System.out.println(map2); jedis.hincrBy("hash2", "age", 10); map2 = jedis.hgetAll("hash2"); System.out.println(map2); &#125; @Test //判断hash是否存在某个值 public void fun5() &#123; System.out.println(jedis.hexists("hash2", "username")); System.out.println(jedis.hexists("hash2", "age")); &#125; @Test //获取hash中键值对的个数 public void fun6() &#123; System.out.println(jedis.hlen("hash2")); &#125; // 获取一个hash中所有的key值 @Test public void fun7() &#123; Set&lt;String&gt; hash2 = jedis.hkeys("hash2"); System.out.println(hash2); &#125; // 获取所有的value值 @Test public void fun8() &#123; List&lt;String&gt; hash2 = jedis.hvals("hash2"); System.out.println(hash2); &#125;&#125; List123456789101112131415161718192021222324252627282930313233public void testList()&#123; jedis.flushDB(); System.out.println("===========添加一个list==========="); jedis.lpush("collections", "ArrayList", "Vector", "Stack", "HashMap", "WeakHashMap", "LinkedHashMap"); jedis.lpush("collections", "HashSet"); jedis.lpush("collections", "TreeSet"); jedis.lpush("collections", "TreeMap"); System.out.println("collections的内容："+jedis.lrange("collections", 0, -1));//-1代表倒数第一个元素，-2代表倒数第二个元素 System.out.println("collections区间0-3的元素："+jedis.lrange("collections",0,3)); System.out.println("==============================="); // 删除列表指定的值 ，第二个参数为删除的个数（有重复时），后add进去的值先被删，类似于出栈 System.out.println("删除指定元素个数："+jedis.lrem("collections", 2, "HashMap")); System.out.println("collections的内容："+jedis.lrange("collections", 0, -1)); System.out.println("删除下表0-3区间之外的元素："+jedis.ltrim("collections", 0, 3)); System.out.println("collections的内容："+jedis.lrange("collections", 0, -1)); System.out.println("collections列表出栈（左端）："+jedis.lpop("collections")); System.out.println("collections的内容："+jedis.lrange("collections", 0, -1)); System.out.println("collections添加元素，从列表右端，与lpush相对应："+jedis.rpush("collections", "EnumMap")); System.out.println("collections的内容："+jedis.lrange("collections", 0, -1)); System.out.println("collections列表出栈（右端）："+jedis.rpop("collections")); System.out.println("collections的内容："+jedis.lrange("collections", 0, -1)); System.out.println("修改collections指定下标1的内容："+jedis.lset("collections", 1, "LinkedArrayList")); System.out.println("collections的内容："+jedis.lrange("collections", 0, -1)); System.out.println("==============================="); System.out.println("collections的长度："+jedis.llen("collections")); System.out.println("获取collections下标为2的元素："+jedis.lindex("collections", 2)); System.out.println("==============================="); jedis.lpush("sortedList", "3","6","2","0","7","4"); System.out.println("sortedList排序前："+jedis.lrange("sortedList", 0, -1)); System.out.println(jedis.sort("sortedList")); System.out.println("sortedList排序后："+jedis.lrange("sortedList", 0, -1));&#125; Set12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** Set集合，和List类的区别就是* set中不会出现重复的数据* 他可以进行聚合操作效率比较高* 其余的操作基本上和list相同** */public class SetTest &#123; public Jedis jedis = JedisPoolUtil.getJedis(); @Test /*添加元素删除元素*/ public void fun()&#123; Long num = jedis.sadd("myset", "a", "a", "b","abc"); System.out.println(num); &#125; @Test /*获得元素*/ public void fun1()&#123; Set&lt;String&gt; myset = jedis.smembers("myset"); System.out.println(myset); &#125; @Test /*移除元素*/ public void fun2()&#123; jedis.srem("myset","a","b"); Set&lt;String&gt; myset = jedis.smembers("myset"); System.out.println(myset); &#125; @Test //判断是否这个set中存在某个值 public void fun3()&#123; Boolean sismember = jedis.sismember("myset", "a"); System.out.println(sismember); &#125; @Test //获得A-B 获得差集合 public void fun4()&#123; jedis.sadd("myset1","123","32","abc","def","123456","sdfasd"); jedis.sadd("myset2","abc","345","123","fda"); Set&lt;String&gt; sdiff = jedis.sdiff("myset1", "myset2"); System.out.println(sdiff); &#125; @Test //获得交集 public void fun5()&#123; Set&lt;String&gt; sinter = jedis.sinter("myset1", "myset2"); System.out.println(sinter); &#125; @Test// 获得并集合 public void fun6()&#123; Set&lt;String&gt; sunion = jedis.sunion("myset1", "myset2"); System.out.println(sunion); &#125; @Test// 成员数量 public void fun7()&#123; System.out.println(jedis.scard("myset1")); &#125; @Test// 获得随机的一个成员 public void fun8()&#123; System.out.println(jedis.srandmember("myset1")); &#125; @Test// 将相差的成员放到一个新的set中同理交集和并集都可以后面均// 加上一个store即可// 并返回新的长度 public void fun9()&#123; System.out.println(jedis.sdiffstore("myset3","myset1","myset2")); System.out.println(jedis.smembers("myset3")); &#125;&#125; SortedSet123456789101112131415161718192021222324/*和set极为的类似，他们是字符串的集合，没有重复的数据差别是sortedset每个成员中都会有一个分数（score）与之关联，redis正是通过分数来为集合中的成员进行从小到大的排序sortedset中数据必须单一但是他的score可以是重复的 */public class SortedsetTest &#123; public Jedis jedis = JedisPoolUtil.getJedis();// 添加元素 @Test public void fun()&#123; jedis.zadd("mysort",100.0, "zhangsan"); jedis.zadd("mysort",200.0,"lisi"); jedis.zadd("mysort",50.0,"wangwu"); Map&lt;String ,Double&gt;map = new HashMap&lt;String ,Double&gt;(); map.put("mutouliu",70.0); jedis.zadd("mysort",map); Set&lt;String&gt; mysort = jedis.zrange("mysort", 0, -1); System.out.println(mysort); Set&lt;String&gt; mysort1 = jedis.zrange("mysort", 1, 2); System.out.println(mysort1); &#125;&#125;]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop中的Secondary NameNode]]></title>
    <url>%2F2018%2F04%2F30%2FHadoop%E4%B8%AD%E7%9A%84Secondary-NameNode%2F</url>
    <content type="text"><![CDATA[在我们深入了解Secondary NameNode之前，我们先来看看NameNode是做什么的。 NameNodeNameNode主要是用来保存HDFS的元数据信息，比如命名空间信息，块信息等。当它运行的时候，这些信息是存在内存中的。但是这些信息也可以持久化到磁盘上。 上面的这张图片展示了NameNode怎么把元数据保存到磁盘上的。这里有两个不同的文件： fsimage - 它是在NameNode启动时对整个文件系统的快照edit logs - 它是在NameNode启动后，对文件系统的改动序列只有在NameNode重启时，edit logs才会合并到fsimage文件中，从而得到一个文件系统的最新快照。但是在产品集群中NameNode是很少重启的，这也意味着当NameNode运行了很长时间后，edit logs文件会变得很大。在这种情况下就会出现下面一些问题： edit logs文件会变的很大，怎么去管理这个文件是一个挑战。NameNode的重启会花费很长时间，因为有很多改动[笔者注:在edit logs中]要合并到fsimage文件上。如果NameNode挂掉了，那我们就丢失了很多改动因为此时的fsimage文件非常旧。因此为了克服这个问题，我们需要一个易于管理的机制来帮助我们减小edit logs文件的大小和得到一个最新的fsimage文件，这样也会减小在NameNode上的压力。这跟Windows的恢复点是非常像的，Windows的恢复点机制允许我们对OS进行快照，这样当系统发生问题时，我们能够回滚到最新的一次恢复点上。 现在我们明白了NameNode的功能和所面临的挑战 - 保持文件系统最新的元数据。那么，这些跟Secondary NameNode又有什么关系呢？ Secondary NameNode其实SecondaryNameNode是hadoop1.x中HDFS HA的一个解决方案，下面我们来看一下SecondaryNameNode工作的流程，如下图： 1.NameNode管理着元数据信息，元数据信息会定期的刷到磁盘中，其中的两个文件是edits即操作日志文件和fsimage即元数据镜像文件，新的操作日志不会立即与fsimage进行合并，也不会刷到NameNode的内存中，而是会先写到edits中(因为合并需要消耗大量的资源)。当edits文件的大小达到一个临界值(默认是64MB)或者间隔一段时间(默认是1小时)的时候checkpoint会触发SecondaryNameNode进行工作。 2.当触发一个checkpoint操作时，NameNode会生成一个新的edits即上图中的edits.new文件，同时SecondaryNameNode会将edits文件和fsimage复制到本地。 3.SecondaryNameNode将本地的fsimage文件加载到内存中，然后再与edits文件进行合并生成一个新的fsimage文件即上图中的Fsimage.ckpt文件。 4.SecondaryNameNode将新生成的Fsimage.ckpt文件复制到NameNode节点。 5.在NameNode结点的edits.new文件和Fsimage.ckpt文件会替换掉原来的edits文件和fsimage文件，至此，刚好是一个轮回即在NameNode中又是edits和fsimage文件了。 6.等待下一次checkpoint触发SecondaryNameNode进行工作，一直这样循环操作。 注：checkpoint触发的条件可以在core-site.xml文件中进行配置，如下： 12345678910111213141516&lt;property&gt; &lt;name&gt;fs.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt; &lt;description&gt; The number of seconds between two periodic checkpoints. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.checkpoint.size&lt;/name&gt; &lt;value&gt;67108864&lt;/value&gt; &lt;description&gt; The size of the current edit log (in bytes) that triggers a periodic checkpoint even if the fs.checkpoint.period hasn't expired. &lt;/description&gt; &lt;/property&gt;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop，namenode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware12中Centos安装及配置]]></title>
    <url>%2F2018%2F04%2F30%2FVMware12%E4%B8%ADCentos%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.安装虚拟机+操作系统 : VMware Centos 2.设置基础环境三要素：IP 主机名 映射(包括本地) 3.网络配置 ：设置DNS解析 PING通外网 4.创建普通用户 5.设置sudo权限 6.禁用防火墙 7.卸载系统自带的Open jdk并配置Oracle jdk 我用的是VM ware12版本 Centos6.x 比较稳定。安装完成后进行相关的配置 虚拟机设置1.打开编辑虚拟机设置，设置相关设备（内存 处理器 硬盘）。网络连接使用NAT模式，用于共享主机的ip地址。 2.配置虚拟网络编辑器。打开VM ware软件上方的编辑选项点击进入。修改VMnet 8 适配器网段地址以及在NAT设置中设置网关。 基本环境的配置修改主机名12345678910查看主机名 在命令行界面输入 hostname临时修改 hostname bigdata-training01.hd.com永久修改/etc/sysconfig/network 如果没有即时生效 命令行输入 init 6 重启虚拟机 vim /etc/sysconfig/network 查看主机名 进入network文件编辑 修改主机名 配置网络1234567891011121314151617181920212223配置IP地址 IP地址:必须与虚拟机NAT模式下分配的IP地址同网段 子网掩码:必须与虚拟机NAT模式下分配的IP地址的子网掩码相同配置连接外网 网关：必须使用NAT模式提供网关，一般默认是.2 DNS：域名解析服务器，建议与网关地址一致，也可以使用一些公共的DNS地址（8.8.8.8）配置网络方式 1.图形化界面配置(比较方便) 2.命令行方式：修改网卡配置文件 vi /etc/sysconfig/networkscripts/ifcfg-eth0 修改完成后重启网络服务 service network restart 这里使用图形化界面配置 网络映射（局域网内）12345修改配置文件：/etc/hosts在文件里加入ip地址 和 对应设置的域名修改windows的本地映射，这样就可以通过本机可以ping通虚拟机的域名 主机名 C:\Windows\System32\drivers\etc\hosts注意，如果host修改失败，可以先copy到桌面，修改完然后替换原有host文件 1（要切换到root下进行操作） 添加删除用户1234useradd usernamepasswd username #给用户设置密码userdel username（要切换到root下进行操作） 添加一个用户aaa 给该用户设置密码 （命令行下输入密码时不会显示 ） 删除用户 并在home目录下删除该用户的文件夹 配置IP地址和主机名映射123vim /etc/hosts进入后在行尾添加 ip地址 域名 主机名 3个值例如：192.168.59.200 bigdata.huadian.com bigdata 添加映射 设置权限输入如下命令 进入后 键盘输入/root搜索 找到下面的语句 并设置用户权限 关闭防火墙可以先看一下防火墙的状态 当前关闭（重启虚拟机后又会失效）：$ sudo service iptables stop永久关闭：$ sudo chkconfig iptables off 出现如下信息说明关闭成功 规划Linux系统目录结构12345678910以系统 /opt 目录为主安装软件包，在该目录下创建以下4个文件夹 /opt /datas 测试数据 /softwares 软件包 /modules 软件安装目录 /tools 开发IDE及工具 安装JDK1234567891011121314151617181920212223242526272829-a. 卸载系统自带的OpenJDK 查看安装的JDK $ sudo rpm -qa|grep java 卸载JDK $ sudo rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.50.1.11.5.el6_3.x86_64 tzdata-java-2012j-1.el6.noarch java-1.7.0-openjdk-1.7.0.9-2.3.4.1.el6_3.x86_64 -b. Linux系统自带上传下载软件 $ sudo yum install -y lrzsz 其中 rz: 上传文件 sz: 表示下载 -c. 安装JDK 在Linux安装JDK类似于Windows绿色软件的安装，直接解压，配置环境变量即可。 - 使用rz上传软件包至/opt/softwares - 解压到/opt/modules $ chmod u+x jdk-8u91-linux-x64.tar.gz ##给压缩包添加执行权限 进行解压 因为我上传后没有执行权限 $ tar -zxf jdk-8u91-linux-x64.tar.gz -C /opt/modules/ - 配置系统环境变量 $ sudo vim /etc/profile 在文件最后增加如下内容： # JAVA HOME export JAVA_HOME=/opt/modules/jdk1.8.0_91 export PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/bin 使其生效 $ source /etc/profile 验证 $ java -version]]></content>
      <categories>
        <category>Centos</category>
      </categories>
      <tags>
        <tag>Centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce中shuffle过程详解]]></title>
    <url>%2F2018%2F04%2F30%2FMapReduce%E4%B8%ADshuffle%E8%BF%87%E7%A8%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[shuffle是MapReduce的核心，map和reduce的中间过程。 Map负责过滤分发，reduce归并整理，从map输出到reduce输入就是shuffle过程。 实现的功能分区决定当前key交给哪个reduce处理 默认：按照key的hash值对reduce的个数取余进行分区 分组将相同key的value合并 排序按照key对每一个keyvalue进行排序，字典排序 过程 map端shufflespill阶段：溢写每一个map task处理的结果会进入环形缓冲区（内存100M） 分区对每一条key进行分区（标上交给哪个reduce） 12345hadoop 1 reduce0hive 1 reduce0spark 1 reduce1hadoop 1 reduce0hbase 1 reduce1 排序按照key排序，将相同分区的数据进行分区内排序 12345hadoop 1 reduce0hadoop 1 reduce0hive 1 reduce0hbase 1 reduce1spark 1 reduce1 溢写当整个缓冲区达到阈值80%，开始进行溢写 12将当前分区排序后的数据写入磁盘变成一个文件file1最终生成多个spill小文件 可以在mapred-site.xml中设置内存的大小和溢写的阈值 12345678910111213141516171819在mapred-site.xml中设置内存的大小 &lt;property&gt; &lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt;在mapred-site.xml中设置内存溢写的阈值 &lt;property&gt; &lt;name&gt;mapreduce.task.io.sort.spill.percent&lt;/name&gt; &lt;value&gt;0.8&lt;/value&gt; &lt;/property&gt; merge:合并将spill生成的多个小文件进行合并 排序：将相同分区的数据进行分区内排序，实现comparator比较器进行比较。最终形成一个文件。 12345678910111213141516171819202122232425file1hadoop 1 reduce0hadoop 1 reduce0hive 1 reduce0hbase 1 reduce1spark 1 reduce1file2hadoop 1 reduce0hadoop 1 reduce0hive 1 reduce0hbase 1 reduce1spark 1 reduce1end_file:hadoop 1 reduce0hadoop 1 reduce0hadoop 1 reduce0hadoop 1 reduce0hive 1 reduce0hive 1 reduce0hbase 1 reduce1hbase 1 reduce1spark 1 reduce1spark 1 reduce1 map task 结束，通知app master，app master通知reduce拉取数据 reduce端shuffle12345678910111213141516171819202122map task1 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hive 1 reduce0 hive 1 reduce0 hbase 1 reduce1 hbase 1 reduce1 spark 1 reduce1 spark 1 reduce1map task2 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hive 1 reduce0 hive 1 reduce0 hbase 1 reduce1 hbase 1 reduce1 spark 1 reduce1 spark 1 reduce1 reduce启动多个线程通过http到每台机器上拉取属于自己分区的数据 12345678910111213reduce0： hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hadoop 1 reduce0 hive 1 reduce0 hive 1 reduce0 hive 1 reduce0 hive 1 reduce0 merge:合并，将每个map task的结果中属于自己的分区数据进行合并 排序：对整体属于我分区的数据进行排序 分组：对相同key的value进行合并，使用comparable完成比较。 12hadoop，list&lt;1,1,1,1,1,1,1,1&gt;hive,list&lt;1,1,1,1&gt; 优化combine在map阶段提前进行一次合并。一般等同于提前执行reduce 1job.setCombinerClass(WCReduce.class); compress压缩中间结果集，减少磁盘IO以及网络IO 压缩配置方式123451.default：所有hadoop中默认的配置项2.site：用于自定义配置文件，如果修改以后必须重启生效3.conf对象配置每个程序的自定义配置4.运行时通过参数实现用户自定义配置bin/yarn jar xx.jar -Dmapreduce.map.output.compress=true -Dmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.Lz4Codec main_class input_path ouput_path 查看本地库支持哪些压缩1bin/hadoop checknative 通过conf配置对象配置压缩123456789101112131415public static void main(String[] args) &#123; Configuration configuration = new Configuration(); //配置map中间结果集压缩 configuration.set("mapreduce.map.output.compress","true"); configuration.set("mapreduce.map.output.compress.codec", "org.apache.hadoop.io.compress.Lz4Codec"); //配置reduce结果集压缩 configuration.set("mapreduce.output.fileoutputformat.compress","true"); configuration.set("mapreduce.output.fileoutputformat.compress.codec", "org.apache.hadoop.io.compress.Lz4Codec"); try &#123; int status = ToolRunner.run(configuration, new MRDriver(), args); System.exit(status); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 编程模板编写【分析网站基本指标UV】程序]]></title>
    <url>%2F2018%2F04%2F30%2FMapReduce-%E7%BC%96%E7%A8%8B%E6%A8%A1%E6%9D%BF%E7%BC%96%E5%86%99%E3%80%90%E5%88%86%E6%9E%90%E7%BD%91%E7%AB%99%E5%9F%BA%E6%9C%AC%E6%8C%87%E6%A0%87UV%E3%80%91%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1.网站基本指标的几个概念PV: page view 浏览量页面的浏览次数，用户每打开一次页面就记录一次。 UV:unique visitor 独立访客数一天内访问某站点的人数（以cookie为例） 但是如果用户把浏览器cookie给删了之后再次访问会影响记录。 VV: visit view 访客的访问次数记录所有访客一天内访问了多少次网站，访客完成访问直到浏览器关闭算一次。 IP：独立ip数指一天内使用不同ip地址的用户访问网站的数量。 2.编写MapReduce编程模板 Driver1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mapreduce;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class MRDriver extends Configured implements Tool &#123; public int run(String[] args) throws Exception &#123; //创建job Job job = Job.getInstance(this.getConf(),"mr-demo"); job.setJarByClass(MRDriver.class); //input 默认从hdfs读取数据 将每一行转换成key-value Path inPath = new Path(args[0]); FileInputFormat.setInputPaths(job,inPath); //map 一行调用一次Map方法 对每一行数据进行分割 job.setMapperClass(null); job.setMapOutputKeyClass(null); job.setMapOutputValueClass(null); //shuffle job.setPartitionerClass(null);//分组 job.setGroupingComparatorClass(null);//分区 job.setSortComparatorClass(null);//排序 //reduce 每有一条key value调用一次reduce方法 job.setReducerClass(null); job.setOutputKeyClass(null); job.setOutputValueClass(null); //output Path outPath = new Path(args[1]); //this.getConf()来自父类 内容为空可以自己set配置信息 FileSystem fileSystem = FileSystem.get(this.getConf()); //如果目录已经存在则删除 if(fileSystem.exists(outPath))&#123; //if path is a directory and set to true fileSystem.delete(outPath,true); &#125; FileOutputFormat.setOutputPath(job, outPath); //submit boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0:1; &#125; public static void main(String[] args) &#123; Configuration configuration = new Configuration(); try &#123; int status = ToolRunner.run(configuration, new MRDriver(), args); System.exit(status); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; Mapper12345678public class MRModelMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; /** * 实现自己的业务逻辑 */ &#125;&#125; Reduce123456789public class MRModelReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; /** * 根据业务需求自己实现 */ &#125;&#125; 3. 统计每个城市的UV数分析需求： UV：unique view 唯一访问数，一个用户记一次 map: key: CityId （城市id） 数据类型： Text value: guid （用户id） 数据类型： Text shuffle: key: CityId value: {guid guid guid..} reduce: key: CityId value: 访问数 即shuffle输出value的集合大小 output: key : CityId value : 访问数 MRDriver.java mapreduce执行过程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package mapreduce;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class MRDriver extends Configured implements Tool &#123; public int run(String[] args) throws Exception &#123; //创建job Job job = Job.getInstance(this.getConf(),"mr-demo"); job.setJarByClass(MRDriver.class); //input 默认从hdfs读取数据 将每一行转换成key-value Path inPath = new Path(args[0]); FileInputFormat.setInputPaths(job,inPath); //map 一行调用一次Map方法 对每一行数据进行分割 job.setMapperClass(MRMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); /* //shuffle job.setPartitionerClass(null);//分组 job.setGroupingComparatorClass(null);//分区 job.setSortComparatorClass();//排序*/ //reduce job.setReducerClass(MRReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //output Path outPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(this.getConf()); if(fileSystem.exists(outPath))&#123; //if path is a directory and set to true fileSystem.delete(outPath,true); &#125; FileOutputFormat.setOutputPath(job, outPath); //submit boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0:1; &#125; public static void main(String[] args) &#123; Configuration configuration = new Configuration(); try &#123; int status = ToolRunner.run(configuration, new MRDriver(), args); System.exit(status); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; MRMapper.java 1234567891011121314151617181920212223242526272829303132package mapreduce;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class MRMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; private Text mapOutKey = new Text(); private Text mapOutKey1 = new Text(); //一行调用一次Map方法 对每一行数据进行分割 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //获得每行的值 String str = value.toString(); //按空格得到每个item String[] items = str.split("\t"); if (items[24]!=null) &#123; this.mapOutKey.set(items[24]); if (items[5]!=null) &#123; this.mapOutKey1.set(items[5]); &#125; &#125; context.write(mapOutKey, mapOutKey1); &#125; &#125; MPReducer.java 1234567891011121314151617181920212223242526package mapreduce;import java.io.IOException;import java.util.HashSet;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MRReducer extends Reducer&lt;Text, Text, Text, IntWritable&gt;&#123; //每有一个key value数据 就执行一次reduce方法 @Override protected void reduce(Text key, Iterable&lt;Text&gt; texts, Reducer&lt;Text, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; HashSet&lt;String&gt; set = new HashSet&lt;String&gt;(); for (Text text : texts) &#123; set.add(text.toString()); &#125; context.write(key,new IntWritable(set.size())); &#125; &#125; 4.MapReduce执行过程理解input：默认从HDFS读取数据12Path inPath = new Path(args[0]);FileInputFormat.setInputPaths(job,inPath); 将每一行数据转换为key-value（分割），这一步由MapReduce框架自动完成。 输出行的偏移量和行的内容 mapper: 分词输出数据过滤，数据补全，字段格式化 输入：input的输出 将分割好的&lt;key,value&gt;对交给用户定义的map方法进行处理，生成新的&lt;key,value&gt;对。 一行调用一次map方法。 统计word中的map： shuffle: 分区，分组，排序输出： 12345&lt;Bye,1&gt;&lt;Hello,1&gt;&lt;World,1,1&gt; 得到map输出的&lt;key,value&gt;对，Mapper会将他们按照key进行排序，得到mapper的最终输出结果。 Reduce ：每一条Keyvalue调用一次reduce方法将相同Key的List，进行相加求和 12345&lt;Bye,1&gt;&lt;Hello,1&gt;&lt;World,2&gt; output：将reduce输出写入hdfs]]></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F21%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>同步github</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
