<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MapReduce 编程模板编写【分析网站基本指标UV】程序]]></title>
    <url>%2F2018%2F04%2F30%2FMapReduce-%E7%BC%96%E7%A8%8B%E6%A8%A1%E6%9D%BF%E7%BC%96%E5%86%99%E3%80%90%E5%88%86%E6%9E%90%E7%BD%91%E7%AB%99%E5%9F%BA%E6%9C%AC%E6%8C%87%E6%A0%87UV%E3%80%91%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1.网站基本指标的几个概念PV: page view 浏览量页面的浏览次数，用户每打开一次页面就记录一次。 UV:unique visitor 独立访客数一天内访问某站点的人数（以cookie为例） 但是如果用户把浏览器cookie给删了之后再次访问会影响记录。 VV: visit view 访客的访问次数记录所有访客一天内访问了多少次网站，访客完成访问直到浏览器关闭算一次。 IP：独立ip数指一天内使用不同ip地址的用户访问网站的数量。 2.编写MapReduce编程模板 Driver1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package mapreduce;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class MRDriver extends Configured implements Tool &#123; public int run(String[] args) throws Exception &#123; //创建job Job job = Job.getInstance(this.getConf(),"mr-demo"); job.setJarByClass(MRDriver.class); //input 默认从hdfs读取数据 将每一行转换成key-value Path inPath = new Path(args[0]); FileInputFormat.setInputPaths(job,inPath); //map 一行调用一次Map方法 对每一行数据进行分割 job.setMapperClass(null); job.setMapOutputKeyClass(null); job.setMapOutputValueClass(null); //shuffle job.setPartitionerClass(null);//分组 job.setGroupingComparatorClass(null);//分区 job.setSortComparatorClass(null);//排序 //reduce 每有一条key value调用一次reduce方法 job.setReducerClass(null); job.setOutputKeyClass(null); job.setOutputValueClass(null); //output Path outPath = new Path(args[1]); //this.getConf()来自父类 内容为空可以自己set配置信息 FileSystem fileSystem = FileSystem.get(this.getConf()); //如果目录已经存在则删除 if(fileSystem.exists(outPath))&#123; //if path is a directory and set to true fileSystem.delete(outPath,true); &#125; FileOutputFormat.setOutputPath(job, outPath); //submit boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0:1; &#125; public static void main(String[] args) &#123; Configuration configuration = new Configuration(); try &#123; int status = ToolRunner.run(configuration, new MRDriver(), args); System.exit(status); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; Mapper12345678public class MRModelMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; /** * 实现自己的业务逻辑 */ &#125;&#125; Reduce123456789public class MRModelReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; /** * 根据业务需求自己实现 */ &#125;&#125; 3. 统计每个城市的UV数分析需求： UV：unique view 唯一访问数，一个用户记一次 map: key: CityId （城市id） 数据类型： Text value: guid （用户id） 数据类型： Text shuffle: key: CityId value: {guid guid guid..} reduce: key: CityId value: 访问数 即shuffle输出value的集合大小 output: key : CityId value : 访问数 MRDriver.java mapreduce执行过程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package mapreduce;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class MRDriver extends Configured implements Tool &#123; public int run(String[] args) throws Exception &#123; //创建job Job job = Job.getInstance(this.getConf(),"mr-demo"); job.setJarByClass(MRDriver.class); //input 默认从hdfs读取数据 将每一行转换成key-value Path inPath = new Path(args[0]); FileInputFormat.setInputPaths(job,inPath); //map 一行调用一次Map方法 对每一行数据进行分割 job.setMapperClass(MRMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); /* //shuffle job.setPartitionerClass(null);//分组 job.setGroupingComparatorClass(null);//分区 job.setSortComparatorClass();//排序*/ //reduce job.setReducerClass(MRReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //output Path outPath = new Path(args[1]); FileSystem fileSystem = FileSystem.get(this.getConf()); if(fileSystem.exists(outPath))&#123; //if path is a directory and set to true fileSystem.delete(outPath,true); &#125; FileOutputFormat.setOutputPath(job, outPath); //submit boolean isSuccess = job.waitForCompletion(true); return isSuccess ? 0:1; &#125; public static void main(String[] args) &#123; Configuration configuration = new Configuration(); try &#123; int status = ToolRunner.run(configuration, new MRDriver(), args); System.exit(status); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; MRMapper.java 1234567891011121314151617181920212223242526272829303132package mapreduce;import java.io.IOException;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class MRMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; private Text mapOutKey = new Text(); private Text mapOutKey1 = new Text(); //一行调用一次Map方法 对每一行数据进行分割 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //获得每行的值 String str = value.toString(); //按空格得到每个item String[] items = str.split("\t"); if (items[24]!=null) &#123; this.mapOutKey.set(items[24]); if (items[5]!=null) &#123; this.mapOutKey1.set(items[5]); &#125; &#125; context.write(mapOutKey, mapOutKey1); &#125; &#125; MPReducer.java 1234567891011121314151617181920212223242526package mapreduce;import java.io.IOException;import java.util.HashSet;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MRReducer extends Reducer&lt;Text, Text, Text, IntWritable&gt;&#123; //每有一个key value数据 就执行一次reduce方法 @Override protected void reduce(Text key, Iterable&lt;Text&gt; texts, Reducer&lt;Text, Text, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123; HashSet&lt;String&gt; set = new HashSet&lt;String&gt;(); for (Text text : texts) &#123; set.add(text.toString()); &#125; context.write(key,new IntWritable(set.size())); &#125; &#125; 4.MapReduce执行过程理解input：默认从HDFS读取数据12Path inPath = new Path(args[0]);FileInputFormat.setInputPaths(job,inPath); 将每一行数据转换为key-value（分割），这一步由MapReduce框架自动完成。 输出行的偏移量和行的内容 mapper: 分词输出数据过滤，数据补全，字段格式化 输入：input的输出 将分割好的&lt;key,value&gt;对交给用户定义的map方法进行处理，生成新的&lt;key,value&gt;对。 一行调用一次map方法。 统计word中的map： shuffle: 分区，分组，排序输出： 12345&lt;Bye,1&gt;&lt;Hello,1&gt;&lt;World,1,1&gt; 得到map输出的&lt;key,value&gt;对，Mapper会将他们按照key进行排序，得到mapper的最终输出结果。 Reduce ：每一条Keyvalue调用一次reduce方法将相同Key的List，进行相加求和 12345&lt;Bye,1&gt;&lt;Hello,1&gt;&lt;World,2&gt; output：将reduce输出写入hdfs]]></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F21%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>同步github</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
